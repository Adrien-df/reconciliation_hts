
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Tutorial : contents &#8212; reconciliation_hts  documentation</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="MAPIE API" href="api.html" />
    <link rel="prev" title="Theoretical Description" href="theoretical_description.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="tutorial">
<span id="id1"></span><h1>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial, we compare the prediction intervals estimated by MAPIE on a
simple, one-dimensional, ground truth function</p>
<div class="math notranslate nohighlight">
\[f(x) = x \sin(x)\]</div>
<p>Throughout this tutorial, we will answer the following questions:</p>
<ul class="simple">
<li><p>How well do the MAPIE strategies capture the aleatoric uncertainty existing in the data?</p></li>
<li><p>How do the prediction intervals estimated by the resampling strategies
evolve for new <em>out-of-distribution</em> data?</p></li>
<li><p>How do the prediction intervals vary between regressor models?</p></li>
</ul>
<p>Throughout this tutorial, we estimate the prediction intervals using
a polynomial function, a boosting model, and a simple neural network.</p>
<p><strong>For practical problems, we advise using the faster CV+ strategies.
For conservative prediction interval estimates, you can alternatively
use the CV-minmax strategies.</strong></p>
<div class="section" id="estimating-the-aleatoric-uncertainty-of-homoscedastic-noisy-data">
<h2>1. Estimating the aleatoric uncertainty of homoscedastic noisy data<a class="headerlink" href="#estimating-the-aleatoric-uncertainty-of-homoscedastic-noisy-data" title="Permalink to this headline">¶</a></h2>
<p>Let’s start by defining the <span class="math notranslate nohighlight">\(x \times \sin(x)\)</span> function and another simple function
that generates one-dimensional data with normal noise uniformely in a given interval.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">x_sinx</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;One-dimensional x*sin(x) function.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_1d_data_with_constant_noise</span><span class="p">(</span><span class="n">funct</span><span class="p">,</span> <span class="n">min_x</span><span class="p">,</span> <span class="n">max_x</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">noise</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate 1D noisy data uniformely from the given function</span>
<span class="sd">    and standard deviation for the noise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">59</span><span class="p">)</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">min_x</span><span class="p">,</span> <span class="n">max_x</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">min_x</span><span class="p">,</span> <span class="n">max_x</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">*</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_mesh</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">funct</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">funct</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">funct</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">y_test</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_mesh</span>
</pre></div>
</div>
<p>We first generate noisy one-dimensional data uniformely on an interval.
Here, the noise is considered as <em>homoscedastic</em>, since it remains constant
over <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">min_x</span><span class="p">,</span> <span class="n">max_x</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">0.5</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_mesh</span> <span class="o">=</span> <span class="n">get_1d_data_with_constant_noise</span><span class="p">(</span>
    <span class="n">x_sinx</span><span class="p">,</span> <span class="n">min_x</span><span class="p">,</span> <span class="n">max_x</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">noise</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Let’s visualize our noisy function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span> <span class="p">;</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_mesh</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<img alt="images/tuto_1.png" class="align-center" src="images/tuto_1.png" />
<p>As mentioned previously, we fit our training data with a simple
polynomial function. Here, we choose a degree equal to 10 so the function
is able to perfectly fit <span class="math notranslate nohighlight">\(x \times \sin(x)\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">degree_polyn</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">polyn_model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;poly&quot;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree_polyn</span><span class="p">)),</span>
        <span class="p">(</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">())</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>We then estimate the prediction intervals for all the strategies very easily with a
<cite>fit</cite> and <cite>predict</cite> process. The prediction interval’s lower and upper bounds
are then saved in a DataFrame. Here, we set an alpha value of 0.05
in order to obtain a 95% confidence for our prediction intervals.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mapie.estimators</span> <span class="kn">import</span> <span class="n">MapieRegressor</span>
<span class="n">STRATEGIES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;naive&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;naive&quot;</span><span class="p">),</span>
    <span class="s2">&quot;jackknife&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;base&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
    <span class="s2">&quot;jackknife_plus&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;plus&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
    <span class="s2">&quot;jackknife_minmax&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;minmax&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
    <span class="s2">&quot;cv&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;base&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="s2">&quot;cv_plus&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;plus&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="s2">&quot;cv_minmax&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;minmax&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
<span class="p">}</span>
<span class="n">y_preds</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">strategy</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="n">STRATEGIES</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">mapie</span> <span class="o">=</span> <span class="n">MapieRegressor</span><span class="p">(</span><span class="n">polyn_model</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">ensemble</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
    <span class="n">mapie</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_preds</span><span class="p">[</span><span class="n">strategy</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapie</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Let’s now compare the confidence intervals with the predicted intervals with obtained
by the Jackknife+, Jackknife-minmax, CV+, and CV-minmax strategies.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_1d_data</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">X_test</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">,</span>
    <span class="n">y_sigma</span><span class="p">,</span>
    <span class="n">y_pred</span><span class="p">,</span>
    <span class="n">y_pred_low</span><span class="p">,</span>
    <span class="n">y_pred_up</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="kc">None</span>
<span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span> <span class="p">;</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_pred_low</span><span class="p">,</span> <span class="n">y_pred_up</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training data&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True confidence intervals&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">-</span> <span class="n">y_sigma</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">+</span> <span class="n">y_sigma</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Prediction intervals&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">title</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategies</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;jackknife_plus&quot;</span><span class="p">,</span> <span class="s2">&quot;jackknife_minmax&quot;</span> <span class="p">,</span> <span class="s2">&quot;cv_plus&quot;</span><span class="p">,</span> <span class="s2">&quot;cv_minmax&quot;</span><span class="p">]</span>
<span class="n">n_figs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">strategies</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">coords</span> <span class="o">=</span> <span class="p">[</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="k">for</span> <span class="n">strategy</span><span class="p">,</span> <span class="n">coord</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">strategies</span><span class="p">,</span> <span class="n">coords</span><span class="p">):</span>
    <span class="n">plot_1d_data</span><span class="p">(</span>
        <span class="n">X_train</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">y_train</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">X_test</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">y_mesh</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="mf">1.96</span><span class="o">*</span><span class="n">noise</span><span class="p">,</span>
        <span class="n">y_preds</span><span class="p">[</span><span class="n">strategy</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">y_preds</span><span class="p">[</span><span class="n">strategy</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">y_preds</span><span class="p">[</span><span class="n">strategy</span><span class="p">][:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">coord</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="n">strategy</span>
    <span class="p">)</span>
</pre></div>
</div>
<img alt="images/tuto_2.png" class="align-center" src="images/tuto_2.png" />
<p>At first glance, the four strategies give similar results and the
prediction intervals are very close to the true confidence intervals.
Let’s confirm this by comparing the prediction interval widths over
<span class="math notranslate nohighlight">\(x\)</span> between all strategies.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">strategy</span> <span class="ow">in</span> <span class="n">STRATEGIES</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">[</span><span class="n">strategy</span><span class="p">][:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_preds</span><span class="p">[</span><span class="n">strategy</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mf">1.96</span><span class="o">*</span><span class="mi">2</span><span class="o">*</span><span class="n">noise</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Prediction Interval Width&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">strategies</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;True width&quot;</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
<img alt="images/tuto_3.png" class="align-center" src="images/tuto_3.png" />
<p>As expected, the prediction intervals estimated by the Naive method
are slightly too narrow. The Jackknife, Jackknife+, CV, and CV+ give
similar widths that are very close to the true width. On the other hand,
the widths estimated by Jackknife-minmax and CV-minmax are slightly too
wide. Note that the widths given by the Naive, Jackknife, and CV strategies
are constant since the prediction intervals are estimated upon the
residuals of the training data only.</p>
<p>Let’s now compare the <em>effective</em> coverage, namely the fraction of test
points whose true values lie within the prediction intervals, given by
the different strategies.</p>
<table border="1" class="dataframe">
<thead>
    <tr style="text-align: right;">
    <th></th>
    <th>Coverage</th>
    <th>Mean width</th>
    </tr>
</thead>
<tbody>
    <tr>
    <th>naive</th>
    <td>0.94</td>
    <td>2.00</td>
    </tr>
    <tr>
    <th>jackknife</th>
    <td>0.97</td>
    <td>2.38</td>
    </tr>
    <tr>
    <th>jackknife_plus</th>
    <td>0.97</td>
    <td>2.36</td>
    </tr>
    <tr>
    <th>jackknife_minmax</th>
    <td>0.98</td>
    <td>2.53</td>
    </tr>
    <tr>
    <th>cv</th>
    <td>0.98</td>
    <td>2.42</td>
    </tr>
    <tr>
    <th>cv_plus</th>
    <td>0.97</td>
    <td>2.34</td>
    </tr>
    <tr>
    <th>cv_minmax</th>
    <td>0.98</td>
    <td>2.62</td>
    </tr>
</tbody>
</table><p>All strategies except the Naive one give effective coverage close to the expected
0.95 value (recall that alpha = 0.05), confirming the theoretical garantees.</p>
</div>
<div class="section" id="estimating-the-epistemic-uncertainty-of-out-of-distribution-data">
<h2>2. Estimating the epistemic uncertainty of out-of-distribution data<a class="headerlink" href="#estimating-the-epistemic-uncertainty-of-out-of-distribution-data" title="Permalink to this headline">¶</a></h2>
<p>Let’s now consider one-dimensional data without noise, but normally distributed.
The goal is to explore how the prediction intervals evolve for new data
that lie outside the distribution of the training data in order to see how the strategies
can capture the <em>epistemic</em> uncertainty.
For a comparison of the epistemic and aleatoric uncertainties, please have a look at this
<a class="reference external" href="https://en.wikipedia.org/wiki/Uncertainty_quantification">source</a>.</p>
<p>Lets” start by generating and showing the data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_1d_data_with_normal_distrib</span><span class="p">(</span><span class="n">funct</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">noise</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate noisy 1D data with normal distribution from given function</span>
<span class="sd">    and noise standard deviation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">59</span><span class="p">)</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">mu</span><span class="o">-</span><span class="mi">4</span><span class="o">*</span><span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span><span class="o">+</span><span class="mi">4</span><span class="o">*</span><span class="n">sigma</span><span class="p">,</span> <span class="n">sig</span><span class="o">/</span><span class="mf">20.</span><span class="p">)</span>
    <span class="n">y_train</span><span class="p">,</span> <span class="n">y_mesh</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">funct</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">funct</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">funct</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">y_test</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_mesh</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span> <span class="p">;</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">2</span> <span class="p">;</span> <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">300</span> <span class="p">;</span> <span class="n">noise</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_mesh</span> <span class="o">=</span> <span class="n">get_1d_data_with_normal_distrib</span><span class="p">(</span>
    <span class="n">x_sinx</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">noise</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span> <span class="p">;</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<img alt="images/tuto_4.png" class="align-center" src="images/tuto_4.png" />
<p>As before, we estimate the prediction intervals using a polynomial
function of degree 10 and show the results for the Jackknife+ and CV+
strategies.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">STRATEGIES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;naive&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;naive&quot;</span><span class="p">),</span>
    <span class="s2">&quot;jackknife&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;base&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
    <span class="s2">&quot;jackknife_plus&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;plus&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
    <span class="s2">&quot;jackknife_minmax&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;minmax&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
    <span class="s2">&quot;cv&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;base&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="s2">&quot;cv_plus&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;plus&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="s2">&quot;cv_minmax&quot;</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;minmax&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
<span class="p">}</span>
<span class="n">prediction_interval</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">strategy</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="n">STRATEGIES</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">mapie</span> <span class="o">=</span> <span class="n">MapieRegressor</span><span class="p">(</span><span class="n">polyn_model</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">ensemble</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
    <span class="n">mapie</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_preds</span><span class="p">[</span><span class="n">strategy</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapie</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategies</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;jackknife_plus&quot;</span><span class="p">,</span> <span class="s2">&quot;jackknife_minmax&quot;</span> <span class="p">,</span> <span class="s2">&quot;cv_plus&quot;</span><span class="p">,</span> <span class="s2">&quot;cv_minmax&quot;</span><span class="p">]</span>
<span class="n">n_figs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">strategy</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">coords</span> <span class="o">=</span> <span class="p">[</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="k">for</span> <span class="n">strategy</span><span class="p">,</span> <span class="n">coord</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">strategies</span><span class="p">,</span> <span class="n">coords</span><span class="p">):</span>
    <span class="n">plot_1d_data</span><span class="p">(</span>
        <span class="n">X_train</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">y_train</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">X_test</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">y_mesh</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="mf">1.96</span><span class="o">*</span><span class="n">noise</span><span class="p">,</span>
        <span class="n">y_preds</span><span class="p">[</span><span class="n">strategy</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">y_preds</span><span class="p">[</span><span class="n">strategy</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">y_preds</span><span class="p">[</span><span class="n">strategy</span><span class="p">][:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">coord</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="n">strategy</span>
    <span class="p">)</span>
</pre></div>
</div>
<img alt="images/tuto_5.png" class="align-center" src="images/tuto_5.png" />
<p>At first glance, our polynomial function does not give accurate
predictions with respect to the true function when <span class="math notranslate nohighlight">\(|x &gt; 6|\)</span>.
The prediction intervals estimated with the Jackknife+ do not seem to
increase significantly, unlike the CV+ method whose prediction intervals
capture a high uncertainty when <span class="math notranslate nohighlight">\(x &gt; 6\)</span>.</p>
<p>Let’s now compare the prediction interval widths between all strategies.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">strategy</span> <span class="ow">in</span> <span class="n">STRATEGIES</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">[</span><span class="n">strategy</span><span class="p">][:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_preds</span><span class="p">[</span><span class="n">strategy</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mf">1.96</span><span class="o">*</span><span class="mi">2</span><span class="o">*</span><span class="n">noise</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Prediction Interval Width&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="o">*</span><span class="n">STRATEGIES</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;True width&quot;</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
<img alt="images/tuto_6.png" class="align-center" src="images/tuto_6.png" />
<p>The prediction interval widths start to increase exponentially
for <span class="math notranslate nohighlight">\(|x| &gt; 4\)</span> for the Jackknife-minmax, CV+, and CV-minmax
strategies. On the other hand, the prediction intervals estimated by
Jackknife+ remain roughly constant until <span class="math notranslate nohighlight">\(|x| ~ 5\)</span> before
increasing.</p>
<table border="1" class="dataframe">
<thead>
    <tr style="text-align: right;">
    <th></th>
    <th>Coverage</th>
    <th>Mean width</th>
    </tr>
</thead>
<tbody>
    <tr>
    <th>naive</th>
    <td>0.49</td>
    <td>0.01</td>
    </tr>
    <tr>
    <th>jackknife</th>
    <td>0.53</td>
    <td>0.01</td>
    </tr>
    <tr>
    <th>jackknife_plus</th>
    <td>0.53</td>
    <td>0.04</td>
    </tr>
    <tr>
    <th>jackknife_minmax</th>
    <td>0.86</td>
    <td>9.78</td>
    </tr>
    <tr>
    <th>cv</th>
    <td>0.52</td>
    <td>0.01</td>
    </tr>
    <tr>
    <th>cv_plus</th>
    <td>0.81</td>
    <td>9.80</td>
    </tr>
    <tr>
    <th>cv_minmax</th>
    <td>0.86</td>
    <td>9.80</td>
    </tr>
</tbody>
</table><p>In conclusion, the Jackknife-minmax, CV+, and CV-minmax strategies are more
conservative than the Jackknife+ strategy, and tend to result in more
reliable coverages for <em>out-of-distribution</em> data. It is therefore
advised to use the three former strategies for predictions with new
out-of-distribution data.
Note however that there are no theoretical guarantees on the coverage level
for out-of-distribution data.</p>
</div>
<div class="section" id="estimating-the-uncertainty-with-different-sklearn-compatible-regressors">
<h2>3. Estimating the uncertainty with different sklearn-compatible regressors<a class="headerlink" href="#estimating-the-uncertainty-with-different-sklearn-compatible-regressors" title="Permalink to this headline">¶</a></h2>
<p>MAPIE can be used with any kind of sklearn-compatible regressor. Here, we
illustrate this by comparing the prediction intervals estimated by the CV+ method using
different models:</p>
<ul class="simple">
<li><p>the same polynomial function as before.</p></li>
<li><p>a XGBoost model using the Scikit-learn API.</p></li>
<li><p>a simple neural network, a Multilayer Perceptron with three dense layers, using the KerasRegressor wrapper.</p></li>
</ul>
<p>Once again, let’s use our noisy one-dimensional data obtained from a
uniform distribution.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">min_x</span><span class="p">,</span> <span class="n">max_x</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">0.5</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_mesh</span> <span class="o">=</span> <span class="n">get_1d_data_with_constant_noise</span><span class="p">(</span>
    <span class="n">x_sinx</span><span class="p">,</span> <span class="n">min_x</span><span class="p">,</span> <span class="n">max_x</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">noise</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span> <span class="p">;</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_mesh</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<img alt="images/tuto_7.png" class="align-center" src="images/tuto_7.png" />
<p>Let’s then define the models. The boosing model considers 100 shallow trees with a max depth of 2 while
the Multilayer Perceptron has two hidden dense layers with 20 neurons each followed by a relu activation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">scikeras.wrappers</span> <span class="kn">import</span> <span class="n">KerasRegressor</span>
<span class="k">def</span> <span class="nf">mlp</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Two-layer MLP model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
        <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mean_squared_error&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">polyn_model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;poly&quot;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degree_polyn</span><span class="p">)),</span>
        <span class="p">(</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBRegressor</span>
<span class="n">xgb_model</span> <span class="o">=</span> <span class="n">XGBRegressor</span><span class="p">(</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">tree_method</span><span class="o">=</span><span class="s2">&quot;hist&quot;</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">59</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">verbosity</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">nthread</span><span class="o">=-</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">mlp_model</span> <span class="o">=</span> <span class="n">KerasRegressor</span><span class="p">(</span>
    <span class="n">build_fn</span><span class="o">=</span><span class="n">mlp</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Let’s now use MAPIE to estimate the prediction intervals using the CV+ method
and compare their prediction interval.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mapie.estimators</span> <span class="kn">import</span> <span class="n">MapieRegressor</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">polyn_model</span><span class="p">,</span> <span class="n">xgb_model</span><span class="p">,</span> <span class="n">mlp_model</span><span class="p">]</span>
<span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;polyn&quot;</span><span class="p">,</span> <span class="s2">&quot;xgb&quot;</span><span class="p">,</span> <span class="s2">&quot;mlp&quot;</span><span class="p">]</span>
<span class="n">prediction_interval</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">model_names</span><span class="p">,</span> <span class="n">models</span><span class="p">):</span>
    <span class="n">mapie</span> <span class="o">=</span> <span class="n">MapieRegressor</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;plus&quot;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">ensemble</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">mapie</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_preds</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">mapie</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">model_names</span><span class="p">,</span> <span class="n">axs</span><span class="p">):</span>
    <span class="n">plot_1d_data</span><span class="p">(</span>
        <span class="n">X_train</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">y_train</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">X_test</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">y_mesh</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="mf">1.96</span><span class="o">*</span><span class="n">noise</span><span class="p">,</span>
        <span class="n">y_preds</span><span class="p">[</span><span class="n">name</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">y_preds</span><span class="p">[</span><span class="n">name</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">y_preds</span><span class="p">[</span><span class="n">name</span><span class="p">][:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="n">name</span>
    <span class="p">)</span>
</pre></div>
</div>
<img alt="images/tuto_8.png" class="align-center" src="images/tuto_8.png" />
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">model_names</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_preds</span><span class="p">[</span><span class="n">name</span><span class="p">][:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_preds</span><span class="p">[</span><span class="n">name</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mf">1.96</span><span class="o">*</span><span class="mi">2</span><span class="o">*</span><span class="n">noise</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Prediction Interval Width&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">model_names</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;True width&quot;</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
<img alt="images/tuto_9.png" class="align-center" src="images/tuto_9.png" />
<p>As expected with the CV+ method, the prediction intervals are a bit
conservative since they are slightly wider than the true intervals.
However, the CV+ method on the three models gives very promising results
since the prediction intervals closely follow the true intervals with <span class="math notranslate nohighlight">\(x\)</span>.</p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">reconciliation_hts</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quick Start with MAPIE</a></li>
</ul>
<p class="caption"><span class="caption-text">Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="theoretical_description.html">Theoretical Description</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#estimating-the-aleatoric-uncertainty-of-homoscedastic-noisy-data">1. Estimating the aleatoric uncertainty of homoscedastic noisy data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#estimating-the-epistemic-uncertainty-of-out-of-distribution-data">2. Estimating the epistemic uncertainty of out-of-distribution data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#estimating-the-uncertainty-with-different-sklearn-compatible-regressors">3. Estimating the uncertainty with different sklearn-compatible regressors</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api.html">MAPIE API</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="theoretical_description.html" title="previous chapter">Theoretical Description</a></li>
      <li>Next: <a href="api.html" title="next chapter">MAPIE API</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Adrien de Forceville.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.5.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/tutorial.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>